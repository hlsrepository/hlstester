import os
import subprocess
import openai
import random
import re


# Define directory
hlso_base_dir = ".../hlsollm_modal"
co_base_dir = ".../co"
mutate_script = os.path.join(hlso_base_dir, "mutate.py")
diff_dir = os.path.join(hlso_base_dir, "diff")
spectre_dir = os.path.join(hlso_base_dir, "spectre")
gt_dir = os.path.join(hlso_base_dir, "gt")  
vitis_bin = ".../bin"
report_file = os.path.join(hlso_base_dir, "report.txt")

# Create the required directory
os.makedirs(diff_dir, exist_ok=True)
os.makedirs(spectre_dir, exist_ok=True)
os.makedirs(gt_dir, exist_ok=True)

# Global line number offset
global_line_number = {
    "input_a.txt": 1,
    "input_b.txt": 1,
    "add_result.txt": 1,
    "subtract_result.txt": 1,
    "divide.txt": 1,
    "multiply.txt": 1,
    "bitwise_and.txt": 1,
    "bitwise_or.txt": 1,
    "bitwise_xor.txt": 1,
    "mod_result.txt": 1
}

# Types of detection errors
error_types = {
    "input_a truncation": False,
    "input_b truncation": False,
    "add_result overflow": False,
    "divide by zero error": False,
    "divide error": False,
    "multiply_result overflow": False,
    "bitwise_and error": False,
    "bitwise_or error": False,
    "bitwise_xor error": False,
    "mod_result error":  False
}

# Global variables: Store detected errors to avoid repeated writes
global_detected_errors = set()
all_error_types = set(error_types.keys())

def generate_gpt_test_vector(kernel_file, current_dat_file, test_id):

    with open(kernel_file, 'r') as kernel_f:
        kernel_content = kernel_f.read()

    with open(current_dat_file, 'r') as dat_f:
        dat_content = dat_f.read()

    prompt_file = os.path.join(gt_dir, f"prompt_{test_id}.txt")
    action_file = os.path.join(gt_dir, f"action_{test_id}.txt")
    output_file = os.path.join(gt_dir, f"gt{test_id}.dat")

    # Call GPT to generate
    gpt_output = consult_gpt(kernel_content, dat_content, prompt_file)
    with open(action_file, 'w') as action_f:
        action_f.write(gpt_output)

    # Extract the test vector and save it
    test_vector = extract_vector_from_gpt_output(action_file)
    with open(output_file, 'w') as output_f:
        output_f.write(test_vector)

    print(f"GPT-generated test vector saved to {output_file}")

def consult_gpt(kernel_code, dat_content, prompt_file):
    message_content = (
        "You are a professional assistant for generating realistic test vectors for High-Level Synthesis (HLS).\n"
        "The kernel implementation and an example test vector are provided. Generate a new test vector in the same format as the example test vector but with randomized, realistic values.\n"
        "Need to detect HLS-specific behavioral differences such as division by zero, overflow, truncation, parallelization, etc.\n"
        "Here is the kernel code for the automatic gain control:\n"
        "```\n"
        f"{kernel_code}\n"
        "```\n\n"
        "Here is the example test vector (content of 1.dat):\n"
        "```\n"
        f"{dat_content}\n"
        "```\n\n"
        "Generate a new test vector in the same format, ensuring randomness. All generated test vector should be placed between ``` and ``` without other words."
    )

    # Save prompt to file
    with open(prompt_file, 'w') as f:
        f.write(message_content)

    # Call GPT
    client = openai.OpenAI()
    response = client.chat.completions.create(
        model="gpt-4o",
        messages=[
            {"role": "system", "content": "You are an assistant for generating realistic test vectors for HLS."},
            {"role": "user", "content": message_content}
        ]
    )
    return response.choices[0].message.content


def extract_vector_from_gpt_output(gpt_output_file):
    vector_content = []
    capture = False
    with open(gpt_output_file, 'r') as file:
        for line in file:
            stripped_line = line.strip()
            # Flags identifying the beginning and end of a code block
            if stripped_line == "```":
                capture = True
                continue
            elif stripped_line == "```" and capture:
                capture = False
                continue
            # Capture effective content
            if capture:
                vector_content.append(stripped_line)

    if not vector_content:
        raise ValueError(f"No valid test vector found in GPT output: {gpt_output_file}")
    
    return "\n".join(vector_content)

def generate_combined_dat_file(gpt_file, mutation_file, output_file, gpt_ratio, mutate_ratio, total_vectors):
    """
    According to the ratio of test vectors generated by GPT and mutate, merge and generate a new .dat file.
    :param gpt_file: Test vector file path generated by GPT
    :param mutation_file: Test vector file path generated by Mutate
    :param output_file: Output .dat file path
    :param gpt_ratio: Ratio of GPT test cases
    :param mutate_ratio: Ratio of Mutate test cases
    :param total_vectors: Total number of test vectors generated in each round
    """
    gpt_lines = []
    mutate_lines = []

    # Read GPT and Mutate data
    if os.path.exists(gpt_file):
        with open(gpt_file, 'r') as f:
            gpt_lines = f.readlines()

    if os.path.exists(mutation_file):
        with open(mutation_file, 'r') as f:
            mutate_lines = f.readlines()

    # Determine the number of generated
    gpt_count = round(total_vectors * gpt_ratio)
    mutate_count = total_vectors - gpt_count

    # Select in proportion, without disrupting the order
    gpt_selected = gpt_lines[:gpt_count]
    mutate_selected = mutate_lines[:mutate_count]

    # Print the specific contents of gpt_selected and mutate_selected here
    print("------ GPT Selected Lines ------")
    for line in gpt_selected:
        print(line.strip())

    print("------ Mutate Selected Lines ------")
    for line in mutate_selected:
        print(line.strip())

    # Ensure the total number is consistent
    total_selected = len(gpt_selected) + len(mutate_selected)
    assert total_selected == total_vectors, \
        f"Expected {total_vectors} vectors, but got {total_selected}"

    # Merge and disrupt order
    combined_lines = gpt_selected + mutate_selected
    random.shuffle(combined_lines)

    # Save the final `.dat` file
    with open(output_file, 'w') as f:
        f.writelines(combined_lines)

    print(f"Generated combined .dat file: {output_file} with {len(gpt_selected)} GPT vectors and {len(mutate_selected)} mutated vectors.")





def create_tcl_script(kernel_path, testbench_path, kernel_src, output_dir):
    # The original code content remains unchanged
    tcl_script_path = f"{hlso_base_dir}/run_vitis.tcl"
    with open(tcl_script_path, 'w') as f:
        f.write(f'''
open_project -reset {output_dir}

# Add kernel source file
add_files {kernel_src}
# Add testbench file
add_files -tb {testbench_path}
set_top addAndDivideAndMultiply

open_solution -reset solution1 -flow_target vitis
set_part  {{xcvu9p-flga2104-2-i}}
create_clock -period 10
set hls_exec 1

# Set simulation command line parameters, pass input file path and output directory
set kernel_path "{kernel_path}"
set output_dir "{output_dir}"
csim_design -argv "$kernel_path $output_dir"

if {{$hls_exec == 1}} {{
    csynth_design
}} elseif {{$hls_exec == 2}} {{
    csynth_design 
    cosim_design -argv "$kernel_path $output_dir"
}} elseif {{$hls_exec == 3}} {{
    csynth_design
    cosim_design  -argv "$kernel_path $output_dir"
    export_design
}} else {{
    csynth_design
}}
quit
''')
    return tcl_script_path


def run_tcl_script(tcl_script_path):
    # The original code content remains unchanged
    vitis_run = os.path.join(vitis_bin, "vitis-run")
    if not os.path.exists(vitis_run):
        raise FileNotFoundError(f"Vitis 'vitis-run' binary not found at {vitis_run}")
    
    command = f"{vitis_run} --mode hls --tcl {tcl_script_path}"
    print(f"Executing command: {command}")
    subprocess.run(command, shell=True, check=True)


def run_cpp_test(test_id, dat_file):
    # The original code content remains unchanged
    cpp_output_dir = f"{co_base_dir}/p{test_id}"
    cpp_kernel_path = f"{co_base_dir}/kernel.cpp"
    cpp_testbench_path = f"{co_base_dir}/kernel_testbench.cpp"

    if not os.path.exists(cpp_kernel_path) or not os.path.exists(cpp_testbench_path):
        raise FileNotFoundError("C++ kernel or testbench source file missing.")

    os.makedirs(cpp_output_dir, exist_ok=True)
    compile_command = f"g++ {cpp_kernel_path} {cpp_testbench_path} -o {cpp_output_dir}/test.out"
    print(f"Compiling C++ test program with command: {compile_command}")
    subprocess.run(compile_command, shell=True, check=True)

    run_command = f"{cpp_output_dir}/test.out {dat_file} {cpp_output_dir} {test_id}"
    print(f"Running C++ test for iteration {test_id} with command: {run_command}")
    subprocess.run(run_command, shell=True, check=True)


def run_hls_test(test_id, dat_file):
    # The original code content remains unchanged
    global global_line_number
    hls_output_dir = f"{hlso_base_dir}/p{test_id}"
    os.makedirs(hls_output_dir, exist_ok=True)

    kernel_src = f"{hlso_base_dir}/kernel.cpp"
    testbench_path = f"{hlso_base_dir}/kernel_testbench.cpp"

    tcl_script_path = create_tcl_script(kernel_path=dat_file, testbench_path=testbench_path, kernel_src=kernel_src, output_dir=hls_output_dir)
    run_tcl_script(tcl_script_path)

    spectre_files = ["input_a.txt", "input_b.txt", "add_result.txt", "subtract_result.txt", "divide.txt", "multiply.txt", "bitwise_and.txt", "bitwise_or.txt",  "bitwise_xor.txt", "mod_result.txt"]
    for fname in spectre_files:
        original_path = os.path.join(hls_output_dir, fname)
        spectre_path = os.path.join(spectre_dir, f"{fname.split('.')[0]}_sp.txt")
        if os.path.exists(original_path):
            with open(original_path, 'r') as original_file, open(spectre_path, 'a') as spectre_file:
                for line_num, line in enumerate(original_file, start=global_line_number[fname]):
                    spectre_file.write(f"{line_num} {line}")
                    global_line_number[fname] += 1


def detect_errors(diff_file, condition, error_type):
    # Logic for detecting errors
    errors_detected = []
    with open(diff_file, 'r') as f:
        for line in f:
            parts = line.strip().split()
            try:
                cpp_result = float(parts[-2])
                hls_result = float(parts[-1])
                variable = " ".join(parts[:-2])
                if condition(cpp_result, hls_result):
                    errors_detected.append(f"{error_type}: Variable: {variable}, C++ Result: {cpp_result}, HLS Result: {hls_result}")
            except ValueError:
                # Ignore rows that cannot be converted to floating point numbers
                continue
    return errors_detected

def compare_results(test_id, dat_file):
    global global_detected_errors
    cpp_output_dir = f"{co_base_dir}/p{test_id}"
    hls_output_dir = f"{hlso_base_dir}/p{test_id}"
    diff_files = [
        ("a.txt", "input_a.txt", "input_a_diff.txt"),
        ("b.txt", "input_b.txt", "input_b_diff.txt"),
        ("a+b.txt", "add_result.txt", "add_result_diff.txt"),
        ("a-b.txt", "subtract_result.txt", "subtract_result_diff.txt"),
        ("multiplyResult.txt", "multiply.txt", "multiply_diff.txt"),
        ("divideResult.txt", "divede.txt", "divide_diff.txt"),
        ("a&b.txt", "bitwise_and.txt", "and_diff.txt"),
        ("a|b.txt", "bitwise_or.txt", "or_diff.txt"),
        ("a^b.txt", "bitwise_xor.txt", "xor_diff.txt"),
        ("/a%b.txt", "mod_result.txt", "mod_diff.txt"),
    ]

    current_detected_errors = set()  #  Current round detected errors

    for cpp_file, hls_file, diff_file in diff_files:
        cpp_path = os.path.join(cpp_output_dir, cpp_file)
        hls_path = os.path.join(hls_output_dir, hls_file)
        diff_path = os.path.join(diff_dir, diff_file)

        if os.path.exists(cpp_path) and os.path.exists(hls_path):
            with open(cpp_path, 'r') as cpp_f, open(hls_path, 'r') as hls_f:
                cpp_lines = cpp_f.readlines()
                hls_lines = hls_f.readlines()
                if len(cpp_lines) != len(hls_lines):
                    print(f"Mismatch in number of lines between {cpp_file} and {hls_file}")
                    continue

                with open(diff_path, 'a') as diff_f:
                    for idx, (cpp_line, hls_line) in enumerate(zip(cpp_lines, hls_lines), start=1):
                        cpp_vals = cpp_line.strip().split()
                        hls_vals = hls_line.strip().split()
                        error_detail = f"{dat_file} {idx} {' '.join(cpp_vals[:-1])} {cpp_vals[-1]} {hls_vals[-1]}"
                        diff_f.write(error_detail + "\n")
                        diff_f.flush()  #  Ensure diff file is updated immediately

                        #  Write immediately to report_file
                        with open(report_file, 'a') as report:
                            report.write(f"{error_detail}\n")
                            report.flush()

                        #  Detect specific error types
                        variable = f"{dat_file} {idx}"
                        if "add_result_diff.txt" in diff_file and variable not in current_detected_errors:
                            current_detected_errors.update(
                                detect_errors(diff_path, lambda c, h: abs(h) < abs(c), "add_result overflow")
                            )

                        if "multiply_diff.txt" in diff_file and variable not in current_detected_errors:
                            current_detected_errors.update(
                                detect_errors(diff_path, lambda c, h: abs(h) < abs(c), "multiply_result overflow")
                            )

                        if "input_a_diff.txt" in diff_file and variable not in current_detected_errors:
                            current_detected_errors.update(
                                detect_errors(diff_path, lambda c, h: abs(h) < abs(c), "input_a truncation")
                            )

                        if "input_b_diff.txt" in diff_file and variable not in current_detected_errors:
                            current_detected_errors.update(
                                detect_errors(diff_path, lambda c, h: abs(h) < abs(c), "input_b truncation")
                            )
                        if "divide_diff.txt" in diff_file and variable not in current_detected_errors:
                            current_detected_errors.update(
                                detect_errors(diff_path, lambda c, h: abs(h) < abs(c), "divide error")
                            )
                        if "subtract_result.txt" in hls_file and variable not in current_detected_errors:
                            current_detected_errors.update(
                                detect_errors(diff_path, lambda c, h: h == 0, "divide by zero error")
                            )
                        
                        if "and_diff.txt" in diff_file and variable not in current_detected_errors:
                            current_detected_errors.update(
                                detect_errors(diff_path, lambda c, h: abs(h) < abs(c), "bitwise_and error")
                            )

                        if "or_diff.txt" in diff_file and variable not in current_detected_errors:
                            current_detected_errors.update(
                                detect_errors(diff_path, lambda c, h: abs(h) < abs(c), "bitwise_or error")
                            )
                        if "xor_diff.txt" in diff_file and variable not in current_detected_errors:
                            current_detected_errors.update(
                                detect_errors(diff_path, lambda c, h: abs(h) < abs(c), "bitwise_xor error")
                            )
                        
                        if "xor_diff.txt" in diff_file and variable not in current_detected_errors:
                            current_detected_errors.update(
                                detect_errors(diff_path, lambda c, h: abs(h) < abs(c), "mod_result error")
                            )

    #  Calculate new errors detected in this iteration
    new_errors = current_detected_errors - global_detected_errors

    #  Update global detection status and write new errors to report file
    with open(report_file, 'a') as report:
        for error in new_errors:
            report.write(f"{error}\n")
        report.flush()

    #  Update global detected errors with current iteration errors
    global_detected_errors.update(current_detected_errors)

    #  Update error detection status
    for error in all_error_types:
        if any(error in e for e in global_detected_errors):
            error_types[error] = True

    return error_types

def extract_first_errors(report_file, report_simple_file):
    """
    Extract the first occurrence of each error type from the report file and update the simplified report file.
    :param report_file: Full error report file path
    :param report_simple_file: Simplified error report file path
    """
    #  Define regex pattern to parse error records
    error_pattern = re.compile(r"^(.*?): Variable: (.*?), C\+\+ Result: (.*?), HLS Result: (.*?)$")

    #  Initialize storage for the first occurrence of each error type
    first_occurrences = {}

    #  If report_simple_file exists, load the existing first error records
    if os.path.exists(report_simple_file):
        with open(report_simple_file, 'r') as f:
            current_simple_content = f.read()

        #  Parse the already recorded first errors
        for match in error_pattern.finditer(current_simple_content):
            error_type, variable, cpp_result, hls_result = match.groups()
            if error_type not in first_occurrences:
                first_occurrences[error_type] = {
                    "variable": variable,
                    "cpp_result": cpp_result,
                    "hls_result": hls_result
                }

    #  Read report_file to extract new first error records
    with open(report_file, 'r') as f:
        for line in f:
            match = error_pattern.match(line.strip())
            if match:
                error_type, variable, cpp_result, hls_result = match.groups()
                #  Update the record if this error type has not been recorded yet or if the new variable appears earlier in the log
                if error_type not in first_occurrences or variable < first_occurrences[error_type]['variable']:
                    first_occurrences[error_type] = {
                        "variable": variable,
                        "cpp_result": cpp_result,
                        "hls_result": hls_result
                    }

    #  Sort all first error records by error type and write them to report_simple_file
    sorted_errors = sorted(first_occurrences.items(), key=lambda x: x[0])
    with open(report_simple_file, 'w') as f:
        for error_type, details in sorted_errors:
            f.write(f"First occurrence of {error_type}:\n")
            f.write(f"{error_type}: Variable: {details['variable']}, C++ Result: {details['cpp_result']}, HLS Result: {details['hls_result']}\n\n")
        f.flush()

    print(f"Simplified report updated in: {report_simple_file}")


def run_mutation(input_file, output_file, mutate_ratio):
    """
    Run the mutate.py script and pass in the mutate_ratio parameter.
    Make sure the output file name conforms to the parsing rules of mutate.py and save the mutation record.
    """
    mutation_file = output_file.replace(".dat", "_mutation.dat")  
    sanitized_output_file = output_file.replace("_mutate", "")  
    print(f"Running mutation script to generate {sanitized_output_file} and record to {mutation_file}...")
    subprocess.run(
        ["python3", mutate_script, input_file, sanitized_output_file, str(mutate_ratio)],
        check=True
    )
    print(f"Mutation records saved to: {mutation_file}")



def main():
    test_id = 1
    kernel_file = os.path.join(hlso_base_dir, "kernel.cpp")

    #  Get the initial number of lines in 1.dat file
    initial_dat_file = os.path.join(hlso_base_dir, "1.dat")
    if not os.path.exists(initial_dat_file):
        raise FileNotFoundError(f"Initial test file {initial_dat_file} not found.")

    with open(initial_dat_file, 'r') as f:
        total_vectors = sum(1 for _ in f)

    print(f"Initial total vectors: {total_vectors}")

    #  File paths
    report_simple_file = os.path.join(hlso_base_dir, "report_simple.txt")

     # Define mutate_ratio
    mutate_ratio = 0.7
    gpt_ratio = 1.0 - mutate_ratio

    while True:
        #  Define input and output file paths
        dat_file = os.path.join(hlso_base_dir, f"{test_id}.dat")
        print(f"Running C++ test for iteration {test_id}...")

        #  Run C++ test
        run_cpp_test(test_id, dat_file)

        print(f"Running HLS synthesis for iteration {test_id}...")
        #  Run HLS test
        run_hls_test(test_id, dat_file)

        print(f"Comparing results for iteration {test_id}...")
        #  Compare results, detect new errors, and write to report.txt
        detected = compare_results(test_id, dat_file)

        #  Update simplified error report
        print(f"Updating simplified report...")
        extract_first_errors(report_file, report_simple_file)

        #  Print detected error status
        print(f"Detected errors: {detected}")

        #  Stop iteration if all error types are detected
        if all(detected.values()):
            print("All error types detected. Stopping testing.")
            break


        #  Generate mutated test vector using mutate.py
        mutation_file = os.path.join(hlso_base_dir, f"{test_id}_mutation.dat")
        print(f"Generating mutated test vector for iteration {test_id}...")
        run_mutation(dat_file, mutation_file, mutate_ratio=mutate_ratio)

        #  Generate new test vector using GPT
        gpt_file = os.path.join(gt_dir, f"gt{test_id}.dat")
        print(f"Generating GPT test vector for iteration {test_id}...")
        generate_gpt_test_vector(kernel_file, dat_file, test_id)

        #  Combine to generate the next round test file
        combined_file = os.path.join(hlso_base_dir, f"{test_id + 1}.dat")
        print(f"Combining test vectors for iteration {test_id + 1}...")
        generate_combined_dat_file(
            gpt_file=gpt_file,
            mutation_file=mutation_file,
            output_file=combined_file,
            gpt_ratio=gpt_ratio,  #  GPT test case ratio
            mutate_ratio=mutate_ratio,  #  Mutate test case ratio
            total_vectors=total_vectors
        )

        print(f"Test file for iteration {test_id + 1} generated.")
        test_id += 1

if __name__ == "__main__":
    main()